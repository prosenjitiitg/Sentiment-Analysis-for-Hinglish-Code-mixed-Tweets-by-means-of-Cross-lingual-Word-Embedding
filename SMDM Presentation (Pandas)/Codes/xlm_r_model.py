# -*- coding: utf-8 -*-
"""script_edited.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Td1y_I8hwPtSdCblx__YFYyPJgDTdnm
"""

from google.colab import drive
drive.mount('/content/drive')

# !pip install ktrain

"""# Importing required packages"""

import numpy as np
from matplotlib import pyplot as plt
import re
import string
from sklearn.metrics import f1_score, confusion_matrix
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
from nltk import download
download('stopwords')

# To store the contents of a tweet in an efficient manner
class Data:
    def __init__(self):
        self.uid = None
        self.content = ''
        self.sentiment = ''

"""# Definining some required functions to load and clean the given data"""

# Function to clean (/pre process) a given tweet
def cleanTweet(data):
    # Ding various pre processing steps to clean the contents of the given tweet
    data.content = re.sub(r'\_', '', data.content) # remove underscores
    data.content = re.sub(r'…', '', data.content) # remove elipses/dots
    data.content = re.sub(r'\.', '', data.content) # remove elipses/dots
    data.content = re.sub(r'^RT[\s]+', '', data.content) # remove retweets
    data.content = re.sub("[#@©àâ€¦¥°¤ð¹ÿœ¾¨‡†§‹²¿¸ˆ]", '', data.content) # remove weird symbols
    data.content = data.content.split("http")[0].split('https')[0] # remove http/https
    data.content = ''.join([i for i in data.content if not i.isdigit()]) # remove digits
    data.content = ''.join([word for word in data.content if word not in string.punctuation]) # remove punctuations
    data.content = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True).tokenize(data.content)
    data.content = ' '.join([i for i in data.content]) # convert to string
    return data

# Loading the stopwords form both engish as well as hinglish
def load_stop_words():
    stopwords_english = stopwords.words('english')
    stopwords_hinglish = []
    with open('/content/drive/My Drive/data/hinglish_stopwords.txt','r') as fp:
        while True:
            line = fp.readline()
            if not line:
                break
            stopwords_hinglish.append(line.strip())
    return stopwords_english, stopwords_hinglish

def readFile(filename, test_data=False):
    stemmer_english = PorterStemmer()
    stopwords_english, stopwords_hinglish = load_stop_words()
    all_datas = []
    with open(filename, 'r', encoding="utf8") as fp:
        data = Data()
        last_one = False
        while True:
            line = fp.readline()
            if not line:
                last_one = True
            if len(line.split()) > 1 or last_one==True:
                if last_one==True or line.split()[0] == 'meta':
                    if len(data.content) > 0 or last_one==True:
                        all_datas.append(cleanTweet(data))
                        if last_one==True:
                            break
                        data = Data()
                    data.uid = line.split()[1]
                    data.sentiment = line.split()[2] if test_data==False else None
                else:
                    if line.split()[1] == "Eng":
                        if line.split()[0] not in stopwords_english:
                            data.content += stemmer_english.stem(line.split()[0]) + " "
                    elif line.split()[1] == "Hin":
                        if line.split()[0] not in stopwords_hinglish:
                            data.content += line.split()[0] + " "
                    else:
                        data.content += line.split()[0] + " "
        return all_datas

"""# Defining Method for Showing Results"""

import itertools

def plot_confusion_matrix(cm,title='Confusion Matrix',cmap=plt.cm.Greens):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    classes = ['neutral','positive','negative']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' 
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

def show_results(y_test, y_pred):
    print("F1 Score: ", f1_score(y_test, y_pred, average="weighted"))
    print()
    cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1,2])
    plot_confusion_matrix(cnf_matrix)

"""# ===================================================

### Load the dataset
"""

all_datas = readFile('/content/drive/My Drive/data/train/train_conll.txt')

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID";
os.environ["CUDA_VISIBLE_DEVICES"]="0";

import ktrain
from ktrain import text

x_train = []
y_train = []

for data in all_datas:
  x_train.append(data.content)
  y_train.append(data.sentiment)

len(x_train), len(y_train)

# Use XLM-RoBERTa model
t = text.Transformer('vicgalle/xlm-roberta-large-xnli-anli')
trn = t.preprocess_train(x_train, y_train)
model = t.get_classifier()
learner = ktrain.get_learner(model, train_data=trn, batch_size=6)

learner.fit_onecycle(2e-7, 1)

"""# ===================================================

### Loading Prediction Data
"""

actual_labels_dict = dict()

with open('/content/drive/My Drive/data/test/test_labels_hinglish.txt','r') as fp:
    line = fp.readline()
    while True:
        line = fp.readline()
        if not line:
            break
        actual_labels_dict[line.strip().split(',')[0]] = line.strip().split(',')[1]

all_test_tweets = readFile('/content/drive/My Drive/data/test/Hindi_test_unalbelled_conll_updated.txt',test_data=True)

# each element of all_test_tweets is a tweet object
print(all_test_tweets[0].content, all_test_tweets[0].sentiment, all_test_tweets[0].uid)

for i in range(len(all_test_tweets)):
  all_test_tweets[i].sentiment = actual_labels_dict[all_test_tweets[i].uid]
  
print(all_test_tweets[0].content, all_test_tweets[0].sentiment, all_test_tweets[0].uid)

"""### Getting the Predictions"""

predictor = ktrain.get_predictor(learner.model, preproc=t)
predictions = []
for i in all_test_tweets:
    predictions.append(predictor.predict(i.content))

"""### The Results"""

actual_num = []
for i in all_test_tweets:
    if i.sentiment == "neutral":
        actual_num.append(1)
    elif i.sentiment == "positive":
        actual_num.append(2)
    elif i.sentiment == "negative":
        actual_num.append(0)
        
predictions_num = []
for i in predictions:
    if i == "neutral":
        predictions_num.append(1)
    elif i == "positive":
        predictions_num.append(2)
    elif i == "negative":
        predictions_num.append(0)

show_results(actual_num, predictions_num)



