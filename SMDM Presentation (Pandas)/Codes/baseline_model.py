# -*- coding: utf-8 -*-
"""Baseline1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AqvK3v0c4OC3olNpalXkzYPxlo4hhyk
"""

from google.colab import drive
drive.mount('/gdrive')

# !pip install ktrain

"""# Importing required packages

"""

import numpy as np
from matplotlib import pyplot as plt
import re
import string
from sklearn.metrics import f1_score, confusion_matrix
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
from nltk import download
download('stopwords')

# To store the contents of a tweet in an efficient manner
class Data:
    def __init__(self):
        self.uid = None
        self.content = ''
        self.sentiment = ''

"""# Definining some required functions to load and clean the given data"""

# Function to clean (/pre process) a given tweet
def cleanTweet(data):
    # Doing various pre processing steps to clean the contents of the given tweet
    data.content = re.sub(r'\_', '', data.content) # remove underscores
    data.content = re.sub(r'…', '', data.content) # remove elipses/dots
    data.content = re.sub(r'\.', '', data.content) # remove elipses/dots
    data.content = re.sub(r'^RT[\s]+', '', data.content) # remove retweets
    data.content = re.sub("[#@©àâ€¦¥°¤ð¹ÿœ¾¨‡†§‹²¿¸ˆ]", '', data.content) # remove weird symbols
    data.content = data.content.split("http")[0].split('https')[0] # remove http/https
    data.content = ''.join([i for i in data.content if not i.isdigit()]) # remove digits
    data.content = ''.join([word for word in data.content if word not in string.punctuation]) # remove punctuations
    data.content = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True).tokenize(data.content)
    data.content = ' '.join([i for i in data.content]) # convert to string
    return data

# Loading the stopwords form both engish as well as hinglish
def load_stop_words():
    stopwords_english = stopwords.words('english')
    stopwords_hinglish = []
    with open('/gdrive/My Drive/SMDM/data/hinglish_stopwords.txt','r') as fp:
        while True:
            line = fp.readline()
            if not line:
                break
            stopwords_hinglish.append(line.strip())
    return stopwords_english, stopwords_hinglish

def readFile(filename, test_data=False):
    stemmer_english = PorterStemmer()
    stopwords_english, stopwords_hinglish = load_stop_words()
    all_datas = []
    with open(filename, 'r', encoding="utf8") as fp:
        data = Data()
        last_one = False
        while True:
            line = fp.readline()
            if not line:
                last_one = True
            if len(line.split()) > 1 or last_one==True:
                if last_one==True or line.split()[0] == 'meta':
                    if len(data.content) > 0 or last_one==True:
                        all_datas.append(cleanTweet(data))
                        if last_one==True:
                            break
                        data = Data()
                    data.uid = line.split()[1]
                    data.sentiment = line.split()[2] if test_data==False else None
                else:
                    if line.split()[1] == "Eng":
                        if line.split()[0] not in stopwords_english:
                            data.content += stemmer_english.stem(line.split()[0]) + " "
                    elif line.split()[1] == "Hin":
                        if line.split()[0] not in stopwords_hinglish:
                            data.content += line.split()[0] + " "
                    else:
                        data.content += line.split()[0] + " "
        return all_datas

"""# Defining Method for Showing Results"""

import itertools

def plot_confusion_matrix(cm,title='Confusion Matrix',cmap=plt.cm.Greens):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    classes = ['neutral','positive','negative']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' 
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

def show_results(y_test, y_pred):
    print("F1 Score: ", f1_score(y_test, y_pred, average="weighted"))
    print()
    cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1,2])
    plot_confusion_matrix(cnf_matrix)

"""# ===================================================

### Load the data
"""

all_datas = readFile('/gdrive/My Drive/SMDM/data/train/train_conll.txt')

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID";
os.environ["CUDA_VISIBLE_DEVICES"]="0";

import ktrain
from ktrain import text

x_train = []
y_train = []

for data in all_datas:
  x_train.append(data.content)
  y_train.append(data.sentiment)

len(x_train), len(y_train)

print(x_train)
print(len(x_train))
print(type(x_train))

print(y_train)

for i in range(len(y_train)):
    if y_train[i]=='negative':
        y_train[i] = 0
    elif y_train[i]=='neutral':
        y_train[i] = 1
    elif y_train[i]=='positive':
        y_train[i] = 2

print(y_train)
print(len(y_train))

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import csv
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding ,Bidirectional
from sklearn.model_selection import train_test_split
import torch
from sklearn.model_selection import train_test_split

tokenizer = Tokenizer(num_words=2500,split=' ')
tokenizer.fit_on_texts(x_train)

X= tokenizer.texts_to_sequences(x_train)
X = pad_sequences(X,maxlen=50)

y = np.array(y_train)

vocab_size = 20000
embed_size = 128
epochs = 50

model = Sequential()
model.add(Embedding(vocab_size, embed_size, input_shape = (X.shape[1],)))
model.add(Bidirectional(LSTM(units=128, activation='tanh')))
model.add(Dense(units=64,activation='tanh'))
model.add(Dropout(0.2))
model.add(Dense(units=3, activation='softmax'))

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07), loss="sparse_categorical_crossentropy", metrics = ['accuracy'])
history = model.fit(X, y, epochs=epochs, batch_size=128,verbose=2)

acc = history.history['accuracy']
loss=history.history['loss']
epochs_range = range(epochs)
plt.figure(figsize=(8, 8))
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.grid()
plt.legend(loc='lower right')
plt.title('Training Accuracy')
plt.show()

epochs_range = range(epochs)
plt.figure(figsize=(8, 8))
plt.plot(epochs_range, loss, label='Training Loss')
plt.grid()
plt.legend(loc='lower right')
plt.title('Training Losses')
plt.show()

actual_labels_dict = dict()

with open('/gdrive/My Drive/SMDM/data/test/abcde.txt','r') as fp:
    line = fp.readline()
    while True:
        line = fp.readline()
        if not line:
            break
        actual_labels_dict[line.strip().split(',')[0]] = line.strip().split(',')[1]

all_test_tweets = readFile('/gdrive/My Drive/SMDM/data/test/Hindi_test_unalbelled_conll_updated.txt',test_data=True)

# each element of all_test_tweets is a tweet object
print(all_test_tweets[0].content, all_test_tweets[0].sentiment, all_test_tweets[0].uid)

for i in range(len(all_test_tweets)):
  all_test_tweets[i].sentiment = actual_labels_dict[all_test_tweets[i].uid]
  
print(all_test_tweets[0].content, all_test_tweets[0].sentiment, all_test_tweets[0].uid)

x_test = []
for i in range(len(all_test_tweets)):
  x_test.append(all_test_tweets[i].content)

print(x_test)

tokenizer = Tokenizer(num_words=2500,split=' ')
tokenizer.fit_on_texts(x_test)

Xt = tokenizer.texts_to_sequences(x_test)
Xt = pad_sequences(Xt,maxlen=50)

pred = model.predict(Xt)

actual_num = []
for i in all_test_tweets:
    if i.sentiment == "neutral":
        actual_num.append(1)
    elif i.sentiment == "positive":
        actual_num.append(2)
    elif i.sentiment == "negative":
        actual_num.append(0)

predictions_num = []

for p in pred:
  lst = list(p)
  max_idx = lst.index(max(lst))

  if max_idx == 0: 
    predictions_num.append(0)
  elif max_idx == 1:  
    predictions_num.append(1)
  else:
    predictions_num.append(2)

show_results(actual_num, predictions_num)

from sklearn.metrics import accuracy_score
accuracy_score(actual_num, predictions_num)