# -*- coding: utf-8 -*-
"""preprocessing1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VL05l48xIiN9tuRaaJ_JR2EuuMmuumO-
"""

from google.colab import drive
drive.mount('/gdrive')

"""Importing required packages"""

# !pip install tweet-preprocessor
# !pip install --upgrade gensim
import smart_open
smart_open.open = smart_open.smart_open
import pandas as pd
import preprocessor as p
import re
import gensim.models as gm
import time
import random
from pathlib import Path

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Reading the data

df = pd.read_csv('/gdrive/My Drive/Colab Notebooks/Sentiment Analysis (Improved)/data/train/Train.txt', sep="\t", header=None )
df.head(10)

# !pip install -U torchtext==0.10.0
import torch 
import torchtext
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchtext.legacy.data import Field, TabularDataset, BucketIterator
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# Currently the output is a categorical value. Here we are converting output into numeric value from categorical value

output = []
for i in range (len(df)) :
    # print(str(df[2][i]))
    if (str(df[2][i]) == "negative"):
        output.append(0)
    elif (str(df[2][i]) == "positive"):
        output.append(2)
    elif (str(df[2][i]) == "neutral"):
        output.append(1)
print (len(output))

tweets=[]
temp=[]
n=len(df)
while i<n:
  if df[0][i] == "meta":
    tweets.append(" ".join(temp))
    temp=[]
    i=i+1
  elif str(df[0][i]) == 'nan':
    i=i+1
  elif df[0][i]== "meta" or df[0][i]== "@" or df[0][i]=="#":
    temp.append(df[0][i]+df[0][i+1])
    i+=2
  else:
    temp.append(str(df[0][i]))
    i+=1
tweets.append(" ".join(temp))
tweets.pop(0)

# Remove emojies, reserved words and links
t=[]
for i in range (len(tweets)) :
    t.append(p.clean (tweets[i]))
tweets=t
t=[]
for i in range(len(tweets)):
    if re.search("http[.]*",tweets[i])!=None:
        t.append(tweets[i][:re.search("http[.]*",tweets[i]).span()[0]])
tweets=t

# Removing special symbols / punctuations

import string
cleaned_tweets = []
tokenized_tweets = []
words  = []
for tweet in (tweets):
    for token in tweet.split():
        if token not in string.punctuation:
            words.append(token.lower())
    cleaned_tweets.append(" ".join(words))
    tokenized_tweets.append(words)
    words = []

# Recreate dataframe from the list of cleaned tweets
print(f'Cleaned tweets length :- {len(cleaned_tweets)}')
output=output[:len(cleaned_tweets)]
id = []
for i in range (len(cleaned_tweets)):
    id.append(i)
dict = {'id' :id, 'text':cleaned_tweets, "label":output}
df = pd.DataFrame(dict)
df.to_csv('train.csv', encoding='utf-8',index=False)
df = pd.read_csv('train.csv')
df.dropna()
df.to_csv('train.csv', encoding='utf-8',index=False)
print (df.head())

# Create word2vec embedding from the code-mixed tweets(tokenized form)

from gensim.models import Word2Vec
model_src = Word2Vec(sentences=tokenized_tweets, vector_size=25, window=5, min_count=1, workers=4)
model_src.wv.save_word2vec_format('model_src.model')

import gensim.downloader as api
model_trgt = api.load("glove-twitter-25")
model_trgt.save_word2vec_format('model_trgt.model')

from google.colab import files
files.upload()

!python3 map_embeddings.py --unsupervised model_src.model model_trgt.model mapped_src.model mapped_trgt.model

from google.colab import files
files.download( "mapped_src.model" )

# Declare fields for tweets and labels
# include_lengths tells the RNN how long the actual sequences are
TEXT = Field(lower=True, include_lengths= True)
LABEL = Field(sequential=False, use_vocab=False, dtype=torch.float)

# Map data to fields
fields = [('id',None), ('text', TEXT), ('label', LABEL) ]

# Apply field definition to create torch dataset
dataset = TabularDataset(
        path="train.csv",
        format="CSV",
        fields=fields,
        skip_header=False
        )

# Split data into train, test, validation sets22
(train_data, test_data, valid_data) = dataset.split(split_ratio=[0.8,0.1,0.1])

print("Number of train data: {}".format(len(train_data)))
print("Number of test data: {}".format(len(test_data)))
print("Number of validation data: {}".format(len(valid_data)))


# Vocabulary
TEXT.build_vocab(dataset)
TEXT.vocab.freqs.most_common(10)

BATCH_SIZE = 128

# sort_within_batch sorts all the tensors within a batch by their lengths
train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data),
    device = device,
    batch_size = BATCH_SIZE,
    sort_key = lambda x: len(x.text),
    sort_within_batch = True)

print(vars(train_iterator.dataset[100]))

# Matching the dimensions

print (len(TEXT.vocab))
model_src = gm.KeyedVectors.load_word2vec_format('mapped_src.model')
print (len(model_src))
model_src = torch.FloatTensor(model_src.vectors)
print(model_src.shape)

class LSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, 
                 bidirectional, dropout):
        """
        Define the layers of the module.

        vocab_size - vocabulary size
        embedding_dim - size of the dense word vectors
        hidden_dim - size of the hidden states
        output_dim - number of classes
        n_layers - number of multi-layer RNN
        bidirectional - boolean - use both directions of LSTM
        dropout - dropout probability
        pad_idx -  string representing the pad token
        """
        
        super().__init__()

        # 1. Feed the tweets in the embedding layer
        # padding_idx set to not learn the emedding for the <pad> token - irrelevant to determining sentiment
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # 2. LSTM layer
        # returns the output and a tuple of the final hidden state and final cell state
        self.encoder = nn.LSTM(embedding_dim, 
                               hidden_dim, 
                               num_layers=n_layers,
                               bidirectional=bidirectional,
                               dropout=dropout)
        
        # 3. Fully-connected layer
        # Final hidden state has both a forward and a backward component concatenated together
        # The size of the input to the nn.Linear layer is twice that of the hidden dimension size
        self.predictor = nn.Linear(hidden_dim*2, output_dim)

        # Initialize dropout layer for regularization
        self.dropout = nn.Dropout(dropout)
      
    def forward(self, text, text_lengths):
        """
        The forward method is called when data is fed into the model.

        text - [tweet length, batch size]
        text_lengths - lengths of tweet
        """

        # embedded = [sentence len, batch size, emb dim]
        embedded = self.dropout(self.embedding(text))    

        # Pack the embeddings - cause RNN to only process non-padded elements
        # Speeds up computation
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)

        # output of encoder
        packed_output, (hidden, cell) = self.encoder(packed_embedded)

        # unpack sequence - transform packed sequence to a tensor
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)
 
        # Get the final layer forward and backward hidden states  
        # concat the final forward and backward hidden layers and apply dropout
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))

        # hidden = [batch size, hid dim * num directions]

        return self.predictor(hidden)

# Model Architecture

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 25
HIDDEN_DIM = 256
OUTPUT_DIM = 1
N_LAYERS = 2
BIDIRECTIONAL = True
DROPOUT = 0.5
# Get pad token index from vocab
# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

model = LSTM(INPUT_DIM,
            EMBEDDING_DIM,
            HIDDEN_DIM,
            OUTPUT_DIM,
            N_LAYERS,
            BIDIRECTIONAL,
            DROPOUT)

# Load the cross-lingual embedding

model_pretrained = gm.KeyedVectors.load_word2vec_format('mapped_src.model')
pretrained_embeddings = torch.FloatTensor(model_pretrained.vectors)
print(pretrained_embeddings.shape)

# Replace the initial weights of the embedding layer with the pre-trained embeddings
model.embedding.weight.data.copy_(pretrained_embeddings)

# Adam optimizer used to update the weights
optimizer = optim.Adam(model.parameters(), lr=2e-2)

# Loss function: binary cross entropy with logits
# It restricts the predictions to a number between 0 and 1 using the logit function
# then use the bound scarlar to calculate the loss using binary cross entropy
criterion = nn.BCEWithLogitsLoss()

# Use GPU
model = model.to(device)
criterion = criterion.to(device)

# Helper functions

def batch_accuracy(predictions, label):
    """
    Returns accuracy per batch.

    predictions - float
    label - 0 or 1
    """

    # Round predictions to the closest integer using the sigmoid function
    preds = torch.round(torch.sigmoid(predictions))
    # If prediction is equal to label
    correct = (preds == label).float()
    # Average correct predictions
    accuracy = correct.sum() / len(correct)

    return accuracy

def timer(start_time, end_time):
    """
    Returns the minutes and seconds.
    """

    time = end_time - start_time
    mins = int(time / 60)
    secs = int(time - (mins * 60))

    return mins, secs

def train(model, iterator, optimizer, criterion):
    """
    Function to evaluate training loss and accuracy.

    iterator - train iterator
    """
    
    # Cumulated Training loss
    training_loss = 0.0
    # Cumulated Training accuracy
    training_acc = 0.0
    
    # Set model to training mode
    model.train()
    
    # For each batch in the training iterator
    for batch in iterator:
        
        # 1. Zero the gradients
        optimizer.zero_grad()
        
        # batch.text is a tuple (tensor, len of seq)
        text, text_lengths = batch.text
        
        # 2. Compute the predictions
        predictions = model(text, text_lengths).squeeze(1)
        
        # 3. Compute loss
        loss = criterion(predictions, batch.label)
        
        # Compute accuracy
        accuracy = batch_accuracy(predictions, batch.label)
        
        # 4. Use loss to compute gradients
        loss.backward()
        
        # 5. Use optimizer to take gradient step
        optimizer.step()
        
        training_loss += loss.item()
        training_acc += accuracy.item()
    
    return training_loss / len(iterator), training_acc / len(iterator)

def evaluate(model, iterator, criterion):
    """
    Function to evaluate the loss and accuracy of validation and test sets.

    iterator - validation or test iterator
    """
    
    # Cumulated Training loss
    eval_loss = 0.0
    # Cumulated Training accuracy
    eval_acc = 0
    
    # Set model to evaluation mode
    model.eval()
    
    # Don't calculate the gradients
    with torch.no_grad():
    
        for batch in iterator:

            text, text_lengths = batch.text
            
            predictions = model(text, text_lengths).squeeze(1)
            
            loss = criterion(predictions, batch.label)
            
            accuracy = batch_accuracy(predictions, batch.label)

            eval_loss += loss.item()
            eval_acc += accuracy.item()
        
    return eval_loss / len(iterator), eval_acc / len(iterator)

# Number of epochs
NUM_EPOCHS = 5

# Lowest validation loss
best_valid_loss = float('inf')

for epoch in range(NUM_EPOCHS):

    start_time = time.time()
  
    train_loss, train_acc = train(model, train_iterator, optimizer, criterion) # Evaluate training loss and accuracy
    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion) # Evaluate validation loss and accuracy
    
    end_time = time.time()

    mins, secs = timer(start_time, end_time)
    
    # At each epoch, if the validation loss is the best
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        
        torch.save(model.state_dict(), 'model-small.pt') # Save the parameters of the model

    print("Epoch {}:".format(epoch+1))
    print("\t Total Time: {}m {}s".format(mins, secs))
    print("\t Train Loss {} | Train Accuracy: {}%".format(round(train_loss, 2), round(train_acc*100, 2)))
    print("\t Validation Loss {} | Validation Accuracy: {}%".format(round(valid_loss, 2), round(valid_acc*100, 2)))

import itertools

def plot_confusion_matrix(cm,title='Confusion Matrix',cmap=plt.cm.Greens):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    classes = ['neutral','positive','negative']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' 
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

def show_results(y_test, y_pred):
    print("F1 Score: ", f1_score(y_test, y_pred, average="weighted"))
    print()
    cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1,2])
    plot_confusion_matrix(cnf_matrix)

pred = model.predict(Xt)

actual_num = []
for i in all_test_tweets:
    if i.sentiment == "neutral":
        actual_num.append(1)
    elif i.sentiment == "positive":
        actual_num.append(2)
    elif i.sentiment == "negative":
        actual_num.append(0)

predictions_num = []

for p in pred:
  lst = list(p)
  max_idx = lst.index(max(lst))

  if max_idx == 0: 
    predictions_num.append(0)
  elif max_idx == 1:  
    predictions_num.append(1)
  else:
    predictions_num.append(2)

show_results(actual_num, predictions_num)